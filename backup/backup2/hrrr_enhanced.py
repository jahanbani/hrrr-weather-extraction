#!/usr/bin/env python3
"""
Enhanced HRRR data extraction with improved error handling, monitoring, and validation.
This is a safe enhancement that doesn't modify existing files.
"""

import datetime
import logging
import os
import warnings
from typing import Any, Dict, Optional

import pandas as pd

# Import enhanced utilities
from config_unified import DEFAULT_CONFIG, HRRRConfig
from utils_enhanced import (
    check_disk_space,
    check_memory_usage,
    create_output_directories,
    format_duration,
    log_system_info,
    performance_monitor,
    validate_data_directory,
    validate_inputs,
)

# Suppress fs package deprecation warnings
warnings.filterwarnings(
    "ignore", category=DeprecationWarning, message=".*pkg_resources.declare_namespace.*"
)
warnings.filterwarnings("ignore", category=DeprecationWarning, message=".*fs.*")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("hrrr_extraction_enhanced.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


def extract_specific_locations_enhanced(
    config: Optional[HRRRConfig] = None,
) -> Dict[str, Any]:
    """Enhanced version of specific locations extraction with better error handling and monitoring."""

    if config is None:
        config = DEFAULT_CONFIG

    logger.info("üöÄ ENHANCED SPECIFIC LOCATIONS EXTRACTION")
    logger.info("=" * 50)

    # Log system information
    log_system_info()

    try:
        # Validate inputs
        logger.info("üîç Validating inputs...")
        validate_inputs(config.wind_csv_path, config.solar_csv_path)

        # Check disk space
        logger.info("üíæ Checking disk space...")
        check_disk_space(config.output_base_dir, required_gb=10.0)

        # Create output directories
        logger.info("üìÅ Creating output directories...")
        output_dirs = config.get_output_dirs()
        create_output_directories(output_dirs)

        # Load location data
        logger.info("üìä Loading location data...")

        wind_df = pd.read_csv(config.wind_csv_path)
        solar_df = pd.read_csv(config.solar_csv_path)

        logger.info(
            f"‚úÖ Loaded {len(wind_df)} wind locations from {config.wind_csv_path}"
        )
        logger.info(
            f"‚úÖ Loaded {len(solar_df)} solar locations from {config.solar_csv_path}"
        )

        # Select only required columns
        wind_locations = wind_df[["pid", "lat", "lon"]].copy()
        solar_locations = solar_df[["pid", "lat", "lon"]].copy()

        logger.info(f"üìä Separated locations:")
        logger.info(f"   Wind locations: {len(wind_locations)}")
        logger.info(f"   Solar locations: {len(solar_locations)}")

        # Get the GRIB data path
        from prereise.gather.const import get_grib_data_path

        grib_path = get_grib_data_path()

        if grib_path is None:
            logger.error("‚ùå No GRIB data path found!")
            return None

        # Validate data directory
        logger.info("üîç Validating data directory...")
        if not validate_data_directory(grib_path):
            logger.error(f"‚ùå Invalid data directory: {grib_path}")
            return None

        logger.info(f"‚úÖ Using GRIB path: {grib_path}")

        # Set date range for testing (just one day)
        START = config.start_date
        END = config.end_date

        logger.info("üìã Extraction Parameters:")
        logger.info(f"   Date range: {START.date()} to {END.date()}")
        logger.info(f"   Wind locations: {len(wind_locations)}")
        logger.info(f"   Wind variables: {list(config.wind_selectors.keys())}")
        logger.info(f"   Solar locations: {len(solar_locations)}")
        logger.info(f"   Solar variables: {list(config.solar_selectors.keys())}")
        logger.info(f"   Workers: {config.num_workers}")
        logger.info(f"   Chunk size: {config.chunk_size}")

        # Start performance monitoring
        performance_monitor.start_operation()

        # Import our new optimized single-pass extraction function
        from extract_specific_points_daily_single_pass import (
            extract_specific_points_daily_single_pass,
        )

        logger.info("üöÄ OPTIMIZED SINGLE-PASS EXTRACTION")
        logger.info("=" * 50)
        logger.info("üìä Using single-pass GRIB reading for maximum efficiency")
        logger.info("   - Each GRIB file read only ONCE")
        logger.info("   - All variables extracted simultaneously")
        logger.info("   - Quarter-hourly data (00, 15, 30, 45 minutes)")
        logger.info("   - ~50% faster than previous approach")

        results = {}

        try:
            # Use our new single-pass extraction function
            logger.info("üéØ Using our new optimized extraction function")

            # Extract location-specific data using DAY-BY-DAY PROCESSING
            from extract_specific_points_daily import extract_specific_points_daily

            # Check memory before extraction
            check_memory_usage()

            extraction_result = extract_specific_points_daily_single_pass(
                wind_csv_path=config.wind_csv_path,
                solar_csv_path=config.solar_csv_path,
                START=START,
                END=END,
                DATADIR=grib_path,
                DEFAULT_HOURS_FORECASTED=["0", "1"],  # Only f00 and f01
                wind_selectors=config.wind_selectors,
                solar_selectors=config.solar_selectors,
                wind_output_dir=output_dirs["wind"],
                solar_output_dir=output_dirs["solar"],
                compression="snappy",
                use_parallel=config.use_parallel,
                num_workers=config.num_workers,
                enable_resume=config.enable_resume,
                batch_size=36,  # Process 36 days at a time (one day per CPU for full utilization)
            )


            # End performance monitoring
            performance_monitor.end_operation(
                files_processed=extraction_result.get("files_processed", 0),
                points_extracted=len(wind_locations) + len(solar_locations),
            )

            logger.info("‚úÖ Day-by-day extraction completed!")
            results["extraction"] = extraction_result

        except Exception as e:
            logger.error(f"‚ùå Error during single-pass extraction: {e}")
            import traceback

            traceback.print_exc()
            results["extraction"] = None

        # Show summary
        logger.info("\nüìÅ EXTRACTION SUMMARY")
        logger.info("=" * 40)

        if os.path.exists(output_dirs["wind"]):
            wind_files = [
                f for f in os.listdir(output_dirs["wind"]) if f.endswith(".parquet")
            ]
            logger.info(
                f"üå™Ô∏è  Wind files: {len(wind_files)} files in {output_dirs['wind']}/"
            )

        if os.path.exists(output_dirs["solar"]):
            solar_files = [
                f for f in os.listdir(output_dirs["solar"]) if f.endswith(".parquet")
            ]
            logger.info(
                f"‚òÄÔ∏è  Solar files: {len(solar_files)} files in {output_dirs['solar']}/"
            )

        if results.get("extraction"):
            extraction_result = results["extraction"]
            logger.info(f"üìä Processing Summary:")
            logger.info(f"   Total days: {extraction_result.get('total_days', 'N/A')}")
            logger.info(
                f"   Successful days: {extraction_result.get('successful_days', 'N/A')}"
            )
            logger.info(
                f"   Failed days: {extraction_result.get('failed_days', 'N/A')}"
            )
            logger.info(
                f"   Processing time: {extraction_result.get('processing_time_seconds', 0) / 3600:.1f} hours"
            )
            logger.info(
                f"   Wind locations: {extraction_result.get('wind_locations', 'N/A')}"
            )
            logger.info(
                f"   Solar locations: {extraction_result.get('solar_locations', 'N/A')}"
            )

        # Performance summary
        summary = performance_monitor.get_summary()
        if summary:
            logger.info("üìà Performance Summary:")
            logger.info(
                f"   Total duration: {format_duration(summary['total_duration'])}"
            )
            logger.info(
                f"   Avg throughput: {summary['avg_throughput_files_per_sec']:.2f} files/s"
            )
            logger.info(f"   Peak memory: {summary['peak_memory_gb']:.1f} GB")
            logger.info(f"   Avg CPU: {summary['avg_cpu_utilization']:.1f}%")

        return results

    except Exception as e:
        logger.error(f"‚ùå Extraction failed: {e}")
        import traceback

        traceback.print_exc()
        return None


def extract_full_grid_enhanced(config: Optional[HRRRConfig] = None) -> Dict[str, Any]:
    """Enhanced version of full grid extraction with better error handling and monitoring."""

    if config is None:
        config = DEFAULT_CONFIG

    logger.info("üöÄ ENHANCED FULL GRID EXTRACTION")
    logger.info("=" * 50)

    # Log system information
    log_system_info()

    try:
        # Get the GRIB data path
        from prereise.gather.const import SELECTORS, get_grib_data_path

        grib_path = get_grib_data_path()

        if grib_path is None:
            logger.error("‚ùå No GRIB data path found!")
            logger.error(
                "Please ensure GRIB files are available in one of the expected locations."
            )
            return None

        # Validate data directory
        logger.info("üîç Validating data directory...")
        if not validate_data_directory(grib_path):
            logger.error(f"‚ùå Invalid data directory: {grib_path}")
            return None

        logger.info(f"‚úÖ Using GRIB path: {grib_path}")

        # Define date range for extraction
        START = datetime.datetime(2023, 1, 1, 0, 0, 0)  # Full year start
        END = datetime.datetime(2023, 12, 31, 23, 0, 0)  # Full year end

        logger.info(f"üìÖ Date range: {START.date()} to {END.date()}")
        logger.info(f"üìä Variables: {list(SELECTORS.keys())}")

        # Check disk space for full grid extraction
        logger.info("üíæ Checking disk space for full grid extraction...")
        check_disk_space(
            config.output_base_dir, required_gb=100.0
        )  # Full grid needs more space

        # Create output directories
        output_dirs = config.get_output_dirs()
        create_output_directories(output_dirs)

        # Start performance monitoring
        performance_monitor.start_operation()

        # Use DAY-BY-DAY AGGRESSIVE optimization for maximum performance
        from prereise.gather.winddata.hrrr.calculations import (
            extract_full_grid_day_by_day,
        )

        result = extract_full_grid_day_by_day(
            START=START,
            END=END,
            DATADIR=grib_path,
            DEFAULT_HOURS_FORECASTED=["0", "1"],
            SELECTORS=SELECTORS,
            output_dir=output_dirs["full_grid"],
            use_aggressive_settings=True,  # Use ALL 36 CPUs and 256GB RAM efficiently
            enable_resume=True,  # Enable resume functionality
        )

        # End performance monitoring
        performance_monitor.end_operation(
            files_processed=result.get("total_days", 0) * 24,  # Estimate files
            points_extracted=result.get("data_points", 0),
        )

        if result:
            logger.info(
                f"\nüéâ DAY-BY-DAY AGGRESSIVE extraction completed with status: {result['status']}"
            )
            logger.info(f"üöÄ Performance metrics:")
            logger.info(f"   Total days: {result.get('total_days', 'N/A')}")
            logger.info(f"   Successful days: {result.get('successful_days', 'N/A')}")
            logger.info(f"   Failed days: {result.get('failed_days', 'N/A')}")
            logger.info(
                f"   Processing time: {result.get('processing_time_seconds', 'N/A'):.1f} seconds ({result.get('processing_time_seconds', 0) / 3600:.1f} hours)"
            )
            logger.info(
                f"   Success rate: {result.get('successful_days', 0) / result.get('total_days', 1) * 100:.1f}%"
            )
            if result.get("resume_used"):
                logger.info("‚úÖ Resume functionality was used")
            if result.get("interrupted"):
                logger.info("‚ö†Ô∏è  Process was interrupted. You can resume later.")
        else:
            logger.error("‚ùå DAY-BY-DAY AGGRESSIVE extraction failed")

        # Performance summary
        summary = performance_monitor.get_summary()
        if summary:
            logger.info("üìà Performance Summary:")
            logger.info(
                f"   Total duration: {format_duration(summary['total_duration'])}"
            )
            logger.info(
                f"   Avg throughput: {summary['avg_throughput_files_per_sec']:.2f} files/s"
            )
            logger.info(f"   Peak memory: {summary['peak_memory_gb']:.1f} GB")
            logger.info(f"   Avg CPU: {summary['avg_cpu_utilization']:.1f}%")

        return result

    except Exception as e:
        logger.error(f"‚ùå Full grid extraction failed: {e}")
        import traceback

        traceback.print_exc()
        return None


def main_enhanced():
    """Enhanced main function with better error handling and configuration."""

    logger.info("üöÄ ENHANCED HRRR Data Extraction")
    logger.info("=" * 50)
    logger.info("Choose extraction type by commenting/uncommenting the lines below:")
    logger.info("")

    # Load configuration
    try:
        config = HRRRConfig()
        config.validate()
        logger.info("‚úÖ Configuration loaded and validated")
    except Exception as e:
        logger.error(f"‚ùå Configuration error: {e}")
        return None

    # ============================================================================
    # CHOOSE EXTRACTION TYPE:
    # ============================================================================

    # Option 1: Full Grid Extraction (1.9M+ points)
    # Uncomment the line below to run full grid extraction
    # result = extract_full_grid_enhanced(config)

    # Option 2: Specific Locations Extraction (wind.csv + solar.csv)
    # Uncomment the line below to run specific locations extraction
    result = extract_specific_locations_enhanced(config)

    # ============================================================================
    # END OF CHOICES
    # ============================================================================

    if result:
        logger.info(f"\n‚úÖ Enhanced extraction completed successfully!")

        # Final performance summary
        summary = performance_monitor.get_summary()
        if summary:
            logger.info("üéØ Final Performance Summary:")
            logger.info(f"   Total time: {format_duration(summary['total_duration'])}")
            logger.info(
                f"   Throughput: {summary['avg_throughput_files_per_sec']:.2f} files/s"
            )
            logger.info(f"   Memory peak: {summary['peak_memory_gb']:.1f} GB")
            logger.info(f"   CPU usage: {summary['avg_cpu_utilization']:.1f}%")
    else:
        logger.error(f"\n‚ùå Enhanced extraction failed!")

    return result


if __name__ == "__main__":
    main_enhanced()
