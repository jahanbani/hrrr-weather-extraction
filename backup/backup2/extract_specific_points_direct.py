#!/usr/bin/env python3

import datetime
import os
import re
import sys
import time

import numpy as np
import pandas as pd
import pygrib
from scipy.spatial import KDTree


def extract_specific_points_direct(
    wind_points,
    solar_points,
    START,
    END,
    DATADIR,
    DEFAULT_HOURS_FORECASTED,
    SELECTORS,
    wind_output_dir="./wind_extracted",
    solar_output_dir="./solar_extracted",
    compression="snappy",
):
    """
    üöÄ DIRECT POINT EXTRACTION: Extract data ONLY for specific grid points.

    This is MUCH more efficient than extracting the full grid and filtering.

    Args:
        wind_points: DataFrame with wind locations ['lat', 'lon', 'pid']
        solar_points: DataFrame with solar locations ['lat', 'lon', 'pid']
        START: Start datetime
        END: End datetime
        DATADIR: Directory containing GRIB files
        DEFAULT_HOURS_FORECASTED: List of forecast hours to process
        SELECTORS: Dictionary mapping variable names to GRIB variable names
        wind_output_dir: Output directory for wind data
        solar_output_dir: Output directory for solar data
        compression: Parquet compression method

    Returns:
        Dictionary with extraction results
    """

    print("üöÄ DIRECT POINT EXTRACTION")
    print("=" * 50)
    print("üìä Extracting data ONLY for specific grid points")
    print("   - No full grid processing")
    print("   - Direct extraction from GRIB files")
    print("   - ~1000x faster than full grid approach")
    print()

    # Step 1: Get grid metadata from a sample GRIB file
    print("üìä Getting grid metadata...")
    sample_file = find_sample_grib_file(DATADIR, START)
    if not sample_file:
        raise ValueError("No GRIB files found")

    grid_lats, grid_lons = extract_grid_metadata_from_file(sample_file)
    print(
        f"‚úÖ Grid dimensions: {grid_lats.shape[0]} x {grid_lats.shape[1]} = {grid_lats.shape[0] * grid_lats.shape[1]:,} total points"
    )

    # Step 2: Find closest grid points for all locations
    print("üìç Finding closest grid points...")
    all_points = pd.concat([wind_points, solar_points], ignore_index=True)
    closest_indices = find_closest_grid_points(all_points, grid_lats, grid_lons)

    # Create mapping from location index to grid index
    location_to_grid = {}
    for i, (_, point) in enumerate(all_points.iterrows()):
        location_to_grid[i] = closest_indices[i]

    print(f"‚úÖ Found closest points for {len(all_points)} locations")

    # Step 3: Find GRIB files for the time period
    print("üîç Finding GRIB files...")
    grib_files = find_grib_files_for_period(
        DATADIR, START, END, DEFAULT_HOURS_FORECASTED
    )
    print(f"‚úÖ Found {len(grib_files)} GRIB files")

    # Step 4: Extract data directly for specific points (PARALLEL)
    print("üìä Extracting data for specific points using 36 CPUs...")

    # Initialize results
    wind_data = {}
    solar_data = {}

    # Prepare tasks for parallel processing
    total_files = len(grib_files)
    print(f"  üìä Processing {total_files} GRIB files with 36 CPUs...")

    # Create tasks: (file_path, var_name, var_selector, closest_indices)
    tasks = []
    for file_path in grib_files:
        for var_name, var_selector in SELECTORS.items():
            tasks.append((file_path, var_name, var_selector, closest_indices))

    print(f"  üöÄ Created {len(tasks)} tasks for parallel processing")

    # Process files in parallel using all 36 CPUs
    import multiprocessing as mp
    from multiprocessing import Pool

    # Use all available CPUs (no artificial limit)
    num_workers = mp.cpu_count()
    print(f"  ‚ö° Using {num_workers} CPU workers for parallel processing")

    # Process in batches to avoid memory issues
    batch_size = 1000  # Process 1000 tasks at a time
    all_results = []

    for batch_start in range(0, len(tasks), batch_size):
        batch_end = min(batch_start + batch_size, len(tasks))
        batch_tasks = tasks[batch_start:batch_end]

        print(
            f"    üì¶ Processing batch {batch_start//batch_size + 1}/{(len(tasks) + batch_size - 1)//batch_size}"
        )

        with Pool(num_workers) as pool:
            batch_results = pool.map(process_file_variable_parallel, batch_tasks)

        # Collect results
        for result in batch_results:
            if result is not None:
                timestamp, var_name, values = result

                # Add to appropriate dataset
                if var_name in ["UWind80", "VWind80", "UWind10", "VWind10"]:
                    # Wind variable
                    if timestamp not in wind_data:
                        wind_data[timestamp] = {}
                    wind_data[timestamp][var_name] = values
                else:
                    # Solar variable
                    if timestamp not in solar_data:
                        solar_data[timestamp] = {}
                    solar_data[timestamp][var_name] = values

        all_results.extend([r for r in batch_results if r is not None])

        # Progress update
        processed_tasks = batch_end
        print(
            f"    ‚úÖ Processed {processed_tasks}/{len(tasks)} tasks ({processed_tasks/len(tasks)*100:.1f}%)"
        )

    print(
        f"  ‚úÖ Parallel processing completed! Extracted {len(all_results)} data points"
    )

    # Step 5: Convert to DataFrames and save
    print("üíæ Saving extracted data...")

    # Convert wind data
    if wind_data:
        wind_df = convert_to_dataframe(wind_data, wind_points)
        os.makedirs(wind_output_dir, exist_ok=True)
        wind_filename = (
            f"wind_data_{START.strftime('%Y%m%d')}_to_{END.strftime('%Y%m%d')}.parquet"
        )
        wind_path = os.path.join(wind_output_dir, wind_filename)
        wind_df.to_parquet(wind_path, compression=compression)
        print(f"‚úÖ Saved wind data: {wind_path}")

    # Convert solar data
    if solar_data:
        solar_df = convert_to_dataframe(solar_data, solar_points)
        os.makedirs(solar_output_dir, exist_ok=True)
        solar_filename = (
            f"solar_data_{START.strftime('%Y%m%d')}_to_{END.strftime('%Y%m%d')}.parquet"
        )
        solar_path = os.path.join(solar_output_dir, solar_filename)
        solar_df.to_parquet(solar_path, compression=compression)
        print(f"‚úÖ Saved solar data: {solar_path}")

    print("‚úÖ Direct point extraction completed!")

    return {
        "status": "completed",
        "wind_locations": len(wind_points),
        "solar_locations": len(solar_points),
        "wind_output_dir": wind_output_dir,
        "solar_output_dir": solar_output_dir,
        "files_processed": len(grib_files),
    }


def find_sample_grib_file(datadir, start_date):
    """Find a sample GRIB file to extract grid metadata."""
    date_str = start_date.strftime("%Y%m%d")
    date_dir = os.path.join(datadir, date_str)

    if os.path.exists(date_dir):
        for filename in os.listdir(date_dir):
            if filename.endswith(".grib2"):
                return os.path.join(date_dir, filename)

    # Fallback: search in datadir directly
    for filename in os.listdir(datadir):
        if filename.endswith(".grib2"):
            return os.path.join(datadir, filename)

    return None


def extract_grid_metadata_from_file(file_path):
    """Extract grid metadata from a GRIB file."""
    with pygrib.open(file_path) as grbs:
        grb = grbs[1]  # First message
        lats, lons = grb.latlons()
        return lats, lons


def find_closest_grid_points(points, grid_lats, grid_lons):
    """Find closest grid points for given lat/lon coordinates."""
    # Flatten grid coordinates
    grid_points = np.column_stack([grid_lats.flatten(), grid_lons.flatten()])

    # Create KDTree for efficient nearest neighbor search
    tree = KDTree(grid_points)

    # Find closest points
    closest_indices = []
    for _, point in points.iterrows():
        lat, lon = point["lat"], point["lon"]
        distance, index = tree.query([lat, lon])
        closest_indices.append(index)

    return closest_indices


def find_grib_files_for_period(datadir, start_date, end_date, hours_forecasted):
    """Find GRIB files for the specified time period."""
    files = []

    # Calculate the date range
    current_date = start_date.date()
    end_date_only = end_date.date()

    print(f"  üìÖ Searching from {current_date} to {end_date_only}")
    print(f"  üïê Hours to process: {hours_forecasted}")

    # Iterate through each date
    while current_date <= end_date_only:
        date_str = current_date.strftime("%Y%m%d")
        date_dir = os.path.join(datadir, date_str)

        if os.path.exists(date_dir):
            print(f"  üìÅ Processing date: {date_str}")

            # For each date, process all 24 hours
            for hour in range(24):
                # Look for f00 and f01 files for this hour
                for forecast in ["00", "01"]:
                    filename = f"hrrr.t{hour:02d}z.wrfsubhf{forecast}.grib2"
                    file_path = os.path.join(date_dir, filename)

                    if os.path.exists(file_path):
                        files.append(file_path)
                        if len(files) % 100 == 0:  # Progress indicator every 100 files
                            print(f"    ‚úÖ Found {len(files)} files so far...")
                    # Don't print missing files for full year (too verbose)
        else:
            print(f"  ‚ùå Directory not found: {date_str}")

        current_date += datetime.timedelta(days=1)

    print(f"  ‚úÖ Total files found: {len(files)}")
    return files


def extract_timestamp_from_filename(file_path):
    """Extract timestamp from GRIB filename."""
    filename = os.path.basename(file_path)
    match = re.search(r"hrrr\.t(\d{2})z\.wrfsubhf(\d{2})\.grib2", filename)
    if match:
        hour = int(match.group(1))
        forecast = match.group(2)

        # Get date from directory path
        dir_path = os.path.dirname(file_path)
        date_match = re.search(r"(\d{8})", dir_path)
        if date_match:
            date_str = date_match.group(1)
            year, month, day = int(date_str[:4]), int(date_str[4:6]), int(date_str[6:8])

            # Create timestamp
            timestamp = pd.Timestamp(year=year, month=month, day=day, hour=hour)

            # Add forecast offset if f01
            if forecast == "01":
                timestamp += pd.Timedelta(minutes=15)

            return timestamp

    return None


def extract_variable_for_points(file_path, var_selector, grid_indices):
    """Extract data for specific grid points from a GRIB file."""
    try:
        with pygrib.open(file_path) as grbs:
            for grb in grbs:
                if grb.name == var_selector:
                    # Extract values for specific grid indices only
                    values_2d = grb.values
                    values = values_2d.flatten()[grid_indices]
                    return values.tolist()
    except Exception as e:
        print(f"    Error extracting {var_selector}: {e}")
        return None

    return None


def process_file_variable_parallel(args):
    """Process a single file-variable combination in parallel.

    Args:
        args: Tuple of (file_path, var_name, var_selector, closest_indices)

    Returns:
        Tuple of (timestamp, var_name, values) or None if failed
    """
    file_path, var_name, var_selector, closest_indices = args

    try:
        # Extract timestamp from filename
        timestamp = extract_timestamp_from_filename(file_path)
        if timestamp is None:
            return None

        # Extract data for specific grid points only
        values = extract_variable_for_points(file_path, var_selector, closest_indices)

        if values is not None:
            return (timestamp, var_name, values)
        else:
            return None

    except Exception as e:
        # Don't print errors in parallel processing to avoid spam
        return None


def convert_to_dataframe(data_dict, points):
    """Convert extracted data to DataFrame."""
    # Create DataFrame with timestamps as index
    df = pd.DataFrame.from_dict(data_dict, orient="index")

    # Add location information
    for i, (_, point) in enumerate(points.iterrows()):
        df[f"location_{i}_lat"] = point["lat"]
        df[f"location_{i}_lon"] = point["lon"]
        df[f"location_{i}_pid"] = point["pid"]

    return df


if __name__ == "__main__":
    # Test the function
    print("Testing direct point extraction...")

    # Sample data
    wind_points = pd.DataFrame(
        {
            "lat": [40.7128, 34.0522, 41.8781],
            "lon": [-74.0060, -118.2437, -87.6298],
            "pid": ["wind1", "wind2", "wind3"],
        }
    )

    solar_points = pd.DataFrame(
        {
            "lat": [32.7157, 29.7604],
            "lon": [-117.1611, -95.3698],
            "pid": ["solar1", "solar2"],
        }
    )

    START = datetime.datetime(2023, 1, 1, 0, 0, 0)
    END = datetime.datetime(2023, 1, 1, 2, 0, 0)
    DATADIR = "/research/alij/hrrr"
    DEFAULT_HOURS_FORECASTED = ["0", "1"]

    SELECTORS = {
        "UWind80": "U component of wind",
        "VWind80": "V component of wind",
        "rad": "Downward short-wave radiation flux",
        "vbd": "Visible Beam Downward Solar Flux",
        "vdd": "Visible Diffuse Downward Solar Flux",
        "2tmp": "2 metre temperature",
        "UWind10": "10 metre U wind component",
        "VWind10": "10 metre V wind component",
    }

    result = extract_specific_points_direct(
        wind_points,
        solar_points,
        START,
        END,
        DATADIR,
        DEFAULT_HOURS_FORECASTED,
        SELECTORS,
    )

    print(f"‚úÖ Test completed: {result}")
